{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving environment: \\ "
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "!conda install -c conda-forge folium=0.5.0 --yes\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download the dataset to our project directory and take a look at the data types \n",
    "!wget -O data.csv \"https://opendata.arcgis.com/datasets/5b5c745e0f1f48e7a53acec63a0022ab_0.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data.csv\")\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The X and Y fields denote the longitude and latitude of the collisions. We can visualize the first few non-null collisions on a map.\n",
    "map = folium.Map(location=[47.60, -122.33], zoom_start=12)\n",
    "marker_cluster = MarkerCluster().add_to(map)\n",
    "locations = data[['Y', 'X']][data['Y'].notna()].head(1000)\n",
    "locationlist = locations.values.tolist()\n",
    "for point in range(len(locations)):\n",
    "    folium.Marker(locationlist[point]).add_to(marker_cluster)\n",
    "map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The WEATHER field contains a description of the weather conditions during the time of the collision.\n",
    "data['WEATHER'].value_counts().to_frame('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ROADCOND field describes the condition of the road during the collision.\n",
    "data['ROADCOND'].value_counts().to_frame('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The LIGHTCOND field describes the light conditions during the collision.\n",
    "data['LIGHTCOND'].value_counts().to_frame('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The SPEEDING field classifies collisions based on whether or not speeding was a factor in the collision. Blanks indicate cases where the vehicle was not speeding.\n",
    "data['SPEEDING'].value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The SEVERITYCODE field contains a code that corresponds to the severity of the collision. and SEVERITYDESC contains a detailed description of the severity of the collision.\n",
    "data['SEVERITYCODE'].value_counts().to_frame('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The UNDERINFL field describes whether or not a driver involved was under the influence of drugs or alcohol. The values 0 and N denote that the driver was not under any influence while 1 and Y that they were.\n",
    "data['UNDERINFL'].value_counts().to_frame('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The PERSONCOUNT and VEHCOUNT indicate how many people and vehicles were involved in a collision respectively.\n",
    "data['PERSONCOUNT'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['VEHCOUNT'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The number of pedestrians involved in the collision helps identify severity involved \n",
    "data['PEDCOUNT'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The number of bicycles involved in the collision helps identify severity involved\n",
    "data['PEDCYLCOUNT'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Cleaning and Processing\n",
    "# Removing unwanted cloumns, Checking blanks and duplicates\n",
    "data.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting relevant fields and dropping others.\n",
    "data_clean = data[['X', 'Y', 'WEATHER', 'ROADCOND', 'LIGHTCOND',\n",
    "                   'SPEEDING', 'SEVERITYCODE', 'UNDERINFL',\n",
    "                   'SERIOUSINJURIES', 'FATALITIES', 'INJURIES',\n",
    "                   'PERSONCOUNT', 'PEDCOUNT', 'PEDCYLCOUNT', 'VEHCOUNT']]\n",
    "data_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fixing the SPEEDING field by encoding it to 0 for the blanks and 1 for the Y values.\n",
    "data_clean['SPEEDING'] = data_clean['SPEEDING'].map({'Y': 1})\n",
    "data_clean['SPEEDING'].replace(np.nan, 0, inplace=True)\n",
    "data_clean['SPEEDING'].value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Records containing values as Unknown and Others can be considered as null values. Severity Code of 0 corresponds to unknown severity, which can also be treated as null.\n",
    "data_clean.replace('Unknown', np.nan, inplace=True)\n",
    "data_clean.replace('Other', np.nan, inplace=True)\n",
    "data_clean['SEVERITYCODE'].replace('0', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can quickly have an overview of the dataset and look at the frequency of missings records.\n",
    "sns.heatmap(data_clean.isnull(), cmap='YlGnBu_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We now drop the records having null values in order to clean the data.\n",
    "data_clean.dropna(axis=0, inplace=True)\n",
    "# Visualizing the dataset after dropping the null values shows that there are no more blanks.\n",
    "sns.heatmap(data_clean.isnull(), cmap='YlGnBu_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As the UNDERINFL field had data inconsistency, it is cleaned by converting all N and 0 values to 0 and all Y and 1 values to 1.\n",
    "data_clean['UNDERINFL'] = data_clean['UNDERINFL'].map({'N': 0, '0': 0, 'Y': 1, '1': 1})\n",
    "data_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the distribution of data\n",
    "ax = sns.countplot(data_clean['WEATHER'])\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, \n",
    "                   horizontalalignment='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Road Condition\n",
    "ax = sns.countplot(data_clean['ROADCOND'])\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, \n",
    "                   horizontalalignment='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# light condition\n",
    "ax = sns.countplot(data_clean['LIGHTCOND'])\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, \n",
    "                   horizontalalignment='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Underinfluence of \n",
    "sns.countplot(data_clean['UNDERINFL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using scatter plot to show vehicle and person count involved in accident\n",
    "ax = plt.scatter(data_clean['VEHCOUNT'], data_clean['PERSONCOUNT'])\n",
    "plt.xlabel('VEHCOUNT')\n",
    "plt.ylabel('PERSONCOUNT')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vehicle count and Injuries\n",
    "ax = plt.scatter(data_clean['VEHCOUNT'], data_clean['INJURIES'])\n",
    "plt.xlabel('VEHCOUNT')\n",
    "plt.ylabel('INJURIES')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pedistrian and person count\n",
    "ax = plt.scatter(data_clean['PEDCOUNT'], data_clean['PERSONCOUNT'])\n",
    "plt.xlabel('PEDCOUNT')\n",
    "plt.ylabel('PERSONCOUNT')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the clean data showing corellation \n",
    "sns.heatmap(data_clean.corr(), cmap='YlGnBu_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hot encoding of the WEATHER, ROADCOND, and LIGHTCOND fields as they are categorical.\n",
    "data_clean = pd.concat([data_clean.drop(['WEATHER', 'ROADCOND', 'LIGHTCOND'], axis=1), \n",
    "           pd.get_dummies(data_clean['ROADCOND']),\n",
    "           pd.get_dummies(data_clean['LIGHTCOND']),\n",
    "           pd.get_dummies(data_clean['WEATHER'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffling of the dataset is necessary as it is an unbalanced dataset.\n",
    "data_clean = data_clean.sample(frac=1).reset_index(drop=True)\n",
    "data_clean.head(5).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the correlation among the features of the dataset \n",
    "sns.heatmap(data_clean.corr(), cmap='YlGnBu_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "from sklearn import preprocessing\n",
    "x = data_clean.drop(['SEVERITYCODE'], axis=1)\n",
    "y = data_clean[['SEVERITYCODE']]\n",
    "data_clean_scaled = preprocessing.StandardScaler().fit(x).transform(x)\n",
    "data_clean_scaled[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(data_clean_scaled, y, \n",
    "                                                    test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Modelling and Evaluation\n",
    "# Decision Tree Model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dTreeModel = DecisionTreeClassifier(criterion='entropy', max_depth=5)\n",
    "dTreeModel.fit(x_train, y_train)\n",
    "dTreeModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yHat = dTreeModel.predict(x_test)\n",
    "print(classification_report(y_test, yHat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfcModel = RandomForestClassifier(n_estimators=75)\n",
    "rfcModel.fit(x_train, y_train)\n",
    "yHat = rfcModel.predict(x_test)\n",
    "print(classification_report(y_test, yHat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logRegModel = LogisticRegression(C=0.01)\n",
    "logRegModel.fit(x_train, y_train)\n",
    "logRegModel\n",
    "yHat = logRegModel.predict(x_test)\n",
    "print(classification_report(y_test, yHat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(32, input_dim=x_train.shape[1], activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(4, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer='adam', \n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "history = model.fit(x_train, tf.keras.utils.to_categorical(\n",
    "    y_train['SEVERITYCODE'].map({\n",
    "        '1': 0,\n",
    "        '2': 1,\n",
    "        '2b': 2,\n",
    "        '3': 3\n",
    "    }), dtype='float32'\n",
    "), epochs=num_epochs, batch_size=50, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train = history.history['loss']\n",
    "loss_validation = history.history['val_loss']\n",
    "epochs = range(1, num_epochs + 1)\n",
    "plt.plot(epochs, loss_train, 'g', label='Training')\n",
    "plt.plot(epochs, loss_validation, 'b', label='Validation')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train = history.history['accuracy']\n",
    "acc_validation = history.history['val_accuracy']\n",
    "epochs = range(1, num_epochs + 1)\n",
    "plt.plot(epochs, acc_train, 'g', label='Training')\n",
    "plt.plot(epochs, acc_validation, 'b', label='Validation')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yHat = model.predict(x_test)\n",
    "yPred = [np.argmax(y) for y in yHat]\n",
    "print(classification_report(y_test.SEVERITYCODE.map({\n",
    "        '1': 0,\n",
    "        '2': 1,\n",
    "        '2b': 2,\n",
    "        '3': 3\n",
    "}), yPred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(['DTC', 'RFC', 'LogReg', 'ANN'], [1.,1.,1.,1.])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
